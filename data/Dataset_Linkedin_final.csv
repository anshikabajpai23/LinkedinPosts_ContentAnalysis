Post,Author Role,Date posted,Challenge category,keyword,link,,,,,,extra information
"404 Media came out with an article trying to learn why why attorneys use AI to write their briefs, focusing on lawyers who have been caught by the courts with hallucinated citations in their briefs. They analyzed and summarized cases in Damien Charloton's excellent database. 

They found a mixture of attorneys blaming IT issues, personal and family emergencies, their own poor judgment, tight deadlines, client demands and most often their assistants. 

The article is a bit sensationalist and is not surprising to anyone following hallucinations in the legal field. Though it's good to get perspective on how those outside the legal industry perceive these mistakes. 

Attorneys use AI (like many other professions) to aid in their work. Not all attorneys understand how AI works which leads to errors like hallucinations in briefs. It's not a magic box. Learn how to use AI and always check your citations (whether you use AI or not!).
insightfullike
16
",Founder,sept 29,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/emma-kelly-ab3a5813_404-media-came-out-with-an-article-trying-activity-7378786574957969408-tn30?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"404 Media came out with an article trying to learn why why attorneys use AI to write their briefs, focusing on lawyers who have been caught by the courts with hallucinated citations in their briefs. They analyzed and summarized cases in Damien Charloton's excellent database. 

They found a mixture of attorneys blaming IT issues, personal and family emergencies, their own poor judgment, tight deadlines, client demands and most often their assistants. 

The article is a bit sensationalist and is not surprising to anyone following hallucinations in the legal field. Though it's good to get perspective on how those outside the legal industry perceive these mistakes. 

Attorneys use AI (like many other professions) to aid in their work. Not all attorneys understand how AI works which leads to errors like hallucinations in briefs. It's not a magic box. Learn how to use AI and always check your citations (whether you use AI or not!).
insightfullike
16
",Founder,sept 29,Organizational Resistance,Ai mistakes,https://www.linkedin.com/posts/emma-kelly-ab3a5813_404-media-came-out-with-an-article-trying-activity-7378786574957969408-tn30?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"404 Media came out with an article trying to learn why why attorneys use AI to write their briefs, focusing on lawyers who have been caught by the courts with hallucinated citations in their briefs. They analyzed and summarized cases in Damien Charloton's excellent database. 

They found a mixture of attorneys blaming IT issues, personal and family emergencies, their own poor judgment, tight deadlines, client demands and most often their assistants. 

The article is a bit sensationalist and is not surprising to anyone following hallucinations in the legal field. Though it's good to get perspective on how those outside the legal industry perceive these mistakes. 

Attorneys use AI (like many other professions) to aid in their work. Not all attorneys understand how AI works which leads to errors like hallucinations in briefs. It's not a magic box. Learn how to use AI and always check your citations (whether you use AI or not!).
insightfullike
16
",Founder,sept 29,ai talent shortage,Ai mistakes,https://www.linkedin.com/posts/emma-kelly-ab3a5813_404-media-came-out-with-an-article-trying-activity-7378786574957969408-tn30?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"A YouTube channel used AI to simulate women begging for their lives, before being shot in the head. The channel was not removed until a journalist asked about it.

YouTube recently removed a channel that had been publishing AI-generated videos depicting women being shot in the head. These videos, created using Google’s Veo tool, were disturbingly realistic and followed a consistent formula: a woman pleads for her life while a man looms over her with a gun, then executes her. 

The channel had over 175,000 views before its removal and featured variations such as “Japanese Schoolgirls Shot in Breast,” “Sexy Housewife Shot in Breast,” and “Female Reporter Tragic End.” It even included polls asking viewers to vote on the ethnicity of the next victim, including the use of racial slurs.

This incident raises serious concerns about how digital technologies are being used to reproduce and amplify gendered violence, particularly against women. These videos reflect deeper societal issues about whose suffering is commodified and consumed. The fact that such content was created, shared, and viewed at scale points to a troubling normalisation of gendered violence in online spaces.

There are also questions about consent and representation. While the individuals depicted may not be real, the imagery draws heavily from familiar cultural archetypes, often racialised and sexualised. This kind of content blurs the lines between fiction and harm, and challenges existing legal and ethical frameworks around digital abuse and exploitation.

The platform response was reactive rather than proactive. YouTube only removed the channel after a 404media inquiry, and Google’s Veo allowed the creation of this content despite supposed safeguards. This highlights the limitations of current content moderation systems and the urgent need for stronger accountability mechanisms in the development and deployment of generative AI tools.
",senior lecturer law,sept 27,Privacy Security & Compliance,ai challenges,https://www.linkedin.com/posts/activity-7377860021742682112-F-BQ?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,"name - cassandra mudgway, the post was deleted"
"A YouTube channel used AI to simulate women begging for their lives, before being shot in the head. The channel was not removed until a journalist asked about it.

YouTube recently removed a channel that had been publishing AI-generated videos depicting women being shot in the head. These videos, created using Google’s Veo tool, were disturbingly realistic and followed a consistent formula: a woman pleads for her life while a man looms over her with a gun, then executes her. 

The channel had over 175,000 views before its removal and featured variations such as “Japanese Schoolgirls Shot in Breast,” “Sexy Housewife Shot in Breast,” and “Female Reporter Tragic End.” It even included polls asking viewers to vote on the ethnicity of the next victim, including the use of racial slurs.

This incident raises serious concerns about how digital technologies are being used to reproduce and amplify gendered violence, particularly against women. These videos reflect deeper societal issues about whose suffering is commodified and consumed. The fact that such content was created, shared, and viewed at scale points to a troubling normalisation of gendered violence in online spaces.

There are also questions about consent and representation. While the individuals depicted may not be real, the imagery draws heavily from familiar cultural archetypes, often racialised and sexualised. This kind of content blurs the lines between fiction and harm, and challenges existing legal and ethical frameworks around digital abuse and exploitation.

The platform response was reactive rather than proactive. YouTube only removed the channel after a 404media inquiry, and Google’s Veo allowed the creation of this content despite supposed safeguards. This highlights the limitations of current content moderation systems and the urgent need for stronger accountability mechanisms in the development and deployment of generative AI tools.
",senior lecturer law,sept 27,Data Quality & Bias,ai challenges,https://www.linkedin.com/posts/activity-7377860021742682112-F-BQ?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,"name - cassandra mudgway, the post was deleted"
"A YouTube channel used AI to simulate women begging for their lives, before being shot in the head. The channel was not removed until a journalist asked about it.

YouTube recently removed a channel that had been publishing AI-generated videos depicting women being shot in the head. These videos, created using Google’s Veo tool, were disturbingly realistic and followed a consistent formula: a woman pleads for her life while a man looms over her with a gun, then executes her. 

The channel had over 175,000 views before its removal and featured variations such as “Japanese Schoolgirls Shot in Breast,” “Sexy Housewife Shot in Breast,” and “Female Reporter Tragic End.” It even included polls asking viewers to vote on the ethnicity of the next victim, including the use of racial slurs.

This incident raises serious concerns about how digital technologies are being used to reproduce and amplify gendered violence, particularly against women. These videos reflect deeper societal issues about whose suffering is commodified and consumed. The fact that such content was created, shared, and viewed at scale points to a troubling normalisation of gendered violence in online spaces.

There are also questions about consent and representation. While the individuals depicted may not be real, the imagery draws heavily from familiar cultural archetypes, often racialised and sexualised. This kind of content blurs the lines between fiction and harm, and challenges existing legal and ethical frameworks around digital abuse and exploitation.

The platform response was reactive rather than proactive. YouTube only removed the channel after a 404media inquiry, and Google’s Veo allowed the creation of this content despite supposed safeguards. This highlights the limitations of current content moderation systems and the urgent need for stronger accountability mechanisms in the development and deployment of generative AI tools.
",senior lecturer law,sept 27,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/activity-7377860021742682112-F-BQ?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,"name - cassandra mudgway, the post was deleted"
"After months of using AI coding tools, I can confidently say that using them is a lot like going to a party with an open bar. 🍺

At first, you feel excited because you have a feeling it's going to be a good experience. Your AI assistant is like a bartender who never says no and is always ready to serve up your next idea.

But that’s where things get tricky. Without guidelines or pacing, things can get out of hand fast. It's easy to trust the logic or reasoning behind the suggestions. They make quick decisions that seem fine in the moment, but over time, those small issues pile up. Before you know it, your code is messy, hard to follow, and you’re stuck with the hangover and spending hours cleaning the mess you made.

The lesson? Pace yourself. Take time to plan. Be detailed. Check in often, and clean as you go.

I’m curious if anyone else using AI tools have experienced the same thing and how you approach using AI tools.",Web developer,1 week ago,Data Quality & Bias,Ai tools problems,https://www.linkedin.com/posts/matthew-a-stanley_after-months-of-using-ai-coding-tools-i-activity-7377055227197259777-T8wo?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI adoption isn’t the biggest hurdle for nonprofits. The real shift is mindset.

The best perspective on where AI helps most comes from the people closest to the work. The learning curve is real, but the payoff on the other side is greater: less friction, more time, and more energy for the mission. 

When staff have more time to focus on people — to do “people work” — it is a win-win.

",Founder,sept 29,Organizational Resistance,ai adoption,https://www.linkedin.com/posts/mlewis7175_ai-adoption-isnt-the-biggest-hurdle-for-activity-7378865371400830977-ooQM?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI agents don’t just make mistakes. They make public mistakes.

A VP of AI at a Fortune500 company recently shared how their agent was trusted to handle customer issues. For weeks it worked flawlessly. Then a single bug caused cold, robotic replies to frustrated users. Screenshots spread online, and the backlash turned into a PR nightmare. The agent didn’t get blamed—the VP did.

AI agents need the same oversight as a new hire. The smartest product teams use guardrails like:

-Continuous-Learning-Human-Feedback (CLHF): review for high-stakes customer interactions.
-Automated error monitoring (tracking hallucinations, tone shifts, or policy breaks).
-Customer Centric Metrics: Fine-tuning custom guardrail metrics that scale for real time intervention.

Platforms like Galileo make this easier by flagging errors in real time, so teams can scale without risking trust.

If your AI agent went off-script tomorrow, would you catch it before your customers did? Link to Guardrails below ",Growth Engineer,sept 29,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/hayden-kempton-9293057a_ai-agents-dont-just-make-mistakes-they-activity-7378883565503180800-MkgX?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,AI Talent Shortage,Lack of in-house expertise to develop or,
"AI agents don’t just make mistakes. They make public mistakes.

A VP of AI at a Fortune500 company recently shared how their agent was trusted to handle customer issues. For weeks it worked flawlessly. Then a single bug caused cold, robotic replies to frustrated users. Screenshots spread online, and the backlash turned into a PR nightmare. The agent didn’t get blamed—the VP did.

AI agents need the same oversight as a new hire. The smartest product teams use guardrails like:

-Continuous-Learning-Human-Feedback (CLHF): review for high-stakes customer interactions.
-Automated error monitoring (tracking hallucinations, tone shifts, or policy breaks).
-Customer Centric Metrics: Fine-tuning custom guardrail metrics that scale for real time intervention.

Platforms like Galileo make this easier by flagging errors in real time, so teams can scale without risking trust.

If your AI agent went off-script tomorrow, would you catch it before your customers did? Link to Guardrails below ",Growth Engineer,sept 29,Privacy Security & Compliance,Ai mistakes,https://www.linkedin.com/posts/hayden-kempton-9293057a_ai-agents-dont-just-make-mistakes-they-activity-7378883565503180800-MkgX?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,AI Talent Shortage,Lack of in-house expertise to develop or,
"AI has proven to be invaluable in analyzing extensive build logs to identify the root cause of build errors. This type of analysis is quick and efficient for AI, while it can be a slow process for individuals manually reviewing thousands of lines of output.

Additionally, AI is beneficial in generating example queries and clarifying API usage. While some examples may occasionally be outdated or inaccurate, they generally provide useful guidance to help navigate challenges effectively.",Principal Software engineer,Sept 29,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/vanloonmichael_ai-has-proven-to-be-invaluable-in-analyzing-activity-7378895309432815616-0ER3?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI in recruiting: the promise was “fair, fast, bias-free.”

The reality? Sometimes it’s just bias on autopilot with a cooler dashboard. 

This week, I pulled my posts together into one article—looking at:
 * Where AI delivers real value
 * How résumé filters can go horribly wrong (and how to fix them)
 * Why humans still matter in the loop
 * The big question: Are we using AI to expand opportunity or just automate old barriers?

AI is here to stay—but fairness is still a choice. And if your AI thinks Hogwarts is your #1 talent source, well… we might need to talk. 
",Talent acquisition specialist,2 weeks ago,Data Quality & Bias,Ai pitfalls,https://www.linkedin.com/posts/kristiancarrico_talentacquisition-airecruiting-futureofwork-activity-7372274528787546112-bfm2?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI in recruiting: the promise was “fair, fast, bias-free.”

The reality? Sometimes it’s just bias on autopilot with a cooler dashboard. 

This week, I pulled my posts together into one article—looking at:
 * Where AI delivers real value
 * How résumé filters can go horribly wrong (and how to fix them)
 * Why humans still matter in the loop
 * The big question: Are we using AI to expand opportunity or just automate old barriers?

AI is here to stay—but fairness is still a choice. And if your AI thinks Hogwarts is your #1 talent source, well… we might need to talk. 
",Talent acquisition specialist,2 weeks ago,ai talent shortage,Ai pitfalls,https://www.linkedin.com/posts/kristiancarrico_talentacquisition-airecruiting-futureofwork-activity-7372274528787546112-bfm2?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI is a hot topic in boardrooms, yet many initiatives falter not due to technology, but due to ego-driven leadership. When optics and ambition guide adoption, the outcome is predictable: wasted resources, demotivated teams, and stagnation.

Leaders driven by ego prioritize flashy projects over value. They mistake AI as a trend, neglecting crucial aspects like data quality and regulatory compliance. Instead of fostering collaboration, they prioritize personal gain, viewing AI as a means to cut jobs rather than enhance decision-making.

The key lies in blending AI with human insight, validated by customer feedback. Successful organizations start small, test rigorously, and scale sensibly. They prioritize foundational elements, encourage teamwork, and embed ethics at every level.

Leaders embracing accountability over ego and purpose over politics will unleash AI's true potential. To boards and CEOs: Are you adopting AI for lasting change or merely for show, disregarding customer needs?",vice president,Oct. 1,Organizational Resistance,ai adoption,https://www.linkedin.com/posts/pier-paolo-borgia_executiveleadership-boardleadership-aileadership-activity-7379242397395800064-zSua?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI is a hot topic in boardrooms, yet many initiatives falter not due to technology, but due to ego-driven leadership. When optics and ambition guide adoption, the outcome is predictable: wasted resources, demotivated teams, and stagnation.

Leaders driven by ego prioritize flashy projects over value. They mistake AI as a trend, neglecting crucial aspects like data quality and regulatory compliance. Instead of fostering collaboration, they prioritize personal gain, viewing AI as a means to cut jobs rather than enhance decision-making.

The key lies in blending AI with human insight, validated by customer feedback. Successful organizations start small, test rigorously, and scale sensibly. They prioritize foundational elements, encourage teamwork, and embed ethics at every level.

Leaders embracing accountability over ego and purpose over politics will unleash AI's true potential. To boards and CEOs: Are you adopting AI for lasting change or merely for show, disregarding customer needs?",vice president,Oct. 1,Data Quality & Bias,ai adoption,https://www.linkedin.com/posts/pier-paolo-borgia_executiveleadership-boardleadership-aileadership-activity-7379242397395800064-zSua?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"AI is a hot topic in boardrooms, yet many initiatives falter not due to technology, but due to ego-driven leadership. When optics and ambition guide adoption, the outcome is predictable: wasted resources, demotivated teams, and stagnation.

Leaders driven by ego prioritize flashy projects over value. They mistake AI as a trend, neglecting crucial aspects like data quality and regulatory compliance. Instead of fostering collaboration, they prioritize personal gain, viewing AI as a means to cut jobs rather than enhance decision-making.

The key lies in blending AI with human insight, validated by customer feedback. Successful organizations start small, test rigorously, and scale sensibly. They prioritize foundational elements, encourage teamwork, and embed ethics at every level.

Leaders embracing accountability over ego and purpose over politics will unleash AI's true potential. To boards and CEOs: Are you adopting AI for lasting change or merely for show, disregarding customer needs?",vice president,Oct. 1,Privacy Security & Compliance,ai adoption,https://www.linkedin.com/posts/pier-paolo-borgia_executiveleadership-boardleadership-aileadership-activity-7379242397395800064-zSua?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Artificial intelligence possesses incredible capabilities, from analyzing vast datasets to writing essays and even diagnosing diseases. Its potential is immense, but so are the risks. The true challenge lies in understanding and mitigating these dangers while harnessing AI's power for good. It is crucial to consider the ethical implications and potential consequences as AI becomes more integrated into our lives.",Founder,1 week old,Privacy Security & Compliance,ai challenges,https://www.linkedin.com/posts/thejimanderson_artificialintelligence-aiethics-technology-activity-7376375142593204225-f7nK?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Caught Ai being wrong twice in last week.

It was technically right, but contextually wrong.

This is the edge humans will always have. 

Understanding the big picture… knowing the point and adjusting along the way.

Ai tends to think in binaries in a lot of ways - the law of the lid applies here, it can’t give you a bigger picture than you can conceive.",Tax and consulting managing partner,Oct 2,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/joe-jag-gallegos-cpa-cva-202065a2_caught-ai-being-wrong-twice-in-last-week-activity-7379376696321507328-AACx?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"CEO: “How did this happen? Where’s the tech lead??”
PM: “You fired him and replaced him with a $20/month subscription.”

This is what happens when companies confuse tools with talent.

AI agents can generate code at lightning speed.
But speed without judgment = faster mistakes.

Here’s the problem nobody wants to admit:
AI doesn’t know when your “core logic” is mission-critical.
It can’t push back when requirements are unrealistic.
It won’t raise a red flag when a shortcut puts customers at risk.

That’s what senior engineers do.
That’s why their intuition - built from years of scar tissue - still matters.

The irony is rich:
Leaders think cutting developers saves money.
In reality, it multiplies risk.

Because you didn’t just lose coders - you lost the people who can tell AI when it’s wrong.
💡 The future isn’t AI or humans.
It’s humans who know how to wield AI responsibly.

Fire your best people and hope the tools will think for you? That’s not transformation.
That’s negligence.

♻️ Share this with someone who might find this useful.
👉 Follow Ranjana for AI insights
",Co-founder,sept 29,ai talent shortage,Ai mistakes,https://www.linkedin.com/posts/ranjanasharma_ceo-how-did-this-happen-wheres-the-tech-activity-7378753070194909184-Y_qB?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Unclear ROI,Hard to demonstrate AI’s financial value.,
"CEO: “How did this happen? Where’s the tech lead??”
PM: “You fired him and replaced him with a $20/month subscription.”

This is what happens when companies confuse tools with talent.

AI agents can generate code at lightning speed.
But speed without judgment = faster mistakes.

Here’s the problem nobody wants to admit:
AI doesn’t know when your “core logic” is mission-critical.
It can’t push back when requirements are unrealistic.
It won’t raise a red flag when a shortcut puts customers at risk.

That’s what senior engineers do.
That’s why their intuition - built from years of scar tissue - still matters.

The irony is rich:
Leaders think cutting developers saves money.
In reality, it multiplies risk.

Because you didn’t just lose coders - you lost the people who can tell AI when it’s wrong.
💡 The future isn’t AI or humans.
It’s humans who know how to wield AI responsibly.

Fire your best people and hope the tools will think for you? That’s not transformation.
That’s negligence.

♻️ Share this with someone who might find this useful.
👉 Follow Ranjana for AI insights
",Co-founder,sept 29,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/ranjanasharma_ceo-how-did-this-happen-wheres-the-tech-activity-7378753070194909184-Y_qB?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Unclear ROI,Hard to demonstrate AI’s financial value.,
"David Tollen nails it again. The real risk isn’t just AI hallucination, it’s AI faithfully reproducing bad precedent. The internet is flooded with sloppy templates and misunderstood doctrines that far outnumber well-drafted agreements.

In law, contracts are path dependent: one mistake gets copied, recycled in templates, and eventually amplified by AI. That’s why drafters who know how to spot human mistakes will be the best equipped to supervise machine ones.

I cannot recommend David Tollen’s book The Tech Contracts Handbook enough. 

https://lnkd.in/ewRCT7VM

And David, maybe your next project should be a Tech/SaaS dictionary, so we’re all finally speaking the same language.",General counsel,1 week ago,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/joannavalencia_to-use-ai-tools-well-we-need-to-understand-activity-7377077187872571392-d5hE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Data Quality & Bias,Poor or biased data leads to unreliable AI,
"David Tollen nails it again. The real risk isn’t just AI hallucination, it’s AI faithfully reproducing bad precedent. The internet is flooded with sloppy templates and misunderstood doctrines that far outnumber well-drafted agreements.

In law, contracts are path dependent: one mistake gets copied, recycled in templates, and eventually amplified by AI. That’s why drafters who know how to spot human mistakes will be the best equipped to supervise machine ones.

I cannot recommend David Tollen’s book The Tech Contracts Handbook enough. 

https://lnkd.in/ewRCT7VM

And David, maybe your next project should be a Tech/SaaS dictionary, so we’re all finally speaking the same language.",General counsel,1 week ago,ai talent shortage,Ai mistakes,https://www.linkedin.com/posts/joannavalencia_to-use-ai-tools-well-we-need-to-understand-activity-7377077187872571392-d5hE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Data Quality & Bias,Poor or biased data leads to unreliable AI,
"Discernment.

AI is very useful but with limitations.

Yes, technology is moving fast. And many say the “must learn” AI skill is prompting. Yes, prompting is important, but equally important is discernment: knowing what to keep in human hands.

AI can process information quickly and handle data at scale, but in real estate, judgement, context, and relationships still shape the outcomes that matter most.

The challenge ahead is not simply what AI can do, but what we choose to keep human.",Head of retail investments,Sept 29,Organizational Resistance,AI limitations,https://www.linkedin.com/posts/kelley-cawley-1277521a4_aiinwork-activity-7378837429526417409-6mrX?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Dovetailing off my previous post about 𝐀𝐈 𝐮𝐬𝐞 𝐚𝐧𝐝 𝐭𝐡𝐞 𝐫𝐢𝐬𝐤𝐬 𝐨𝐟 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠. 

A few months ago, I had a conversation with a group of CSOs on this topic. The consensus was: 𝘈𝘐 𝘪𝘴 𝘩𝘦𝘳𝘦, 𝘢𝘯𝘥 𝘢𝘴 𝘮𝘶𝘤𝘩 𝘢𝘴 𝘸𝘦 𝘵𝘳𝘺 𝘵𝘰 𝘳𝘦𝘴𝘵𝘳𝘪𝘤𝘵 𝘪𝘵, 𝘢𝘯𝘢𝘭𝘺𝘴𝘵𝘴 𝘸𝘪𝘭𝘭 𝘴𝘵𝘪𝘭𝘭 𝘧𝘪𝘯𝘥 𝘢 𝘸𝘢𝘺 𝘵𝘰 𝘶𝘴𝘦 𝘪𝘵. So the challenge becomes, how do they ensure analysts are using AI appropriately? 

💡 A few solutions we discussed include:

▪️ 𝐁𝐮𝐢𝐥𝐝𝐢𝐧𝐠 𝐀𝐈 𝐥𝐢𝐭𝐞𝐫𝐚𝐜𝐲 𝐢𝐧𝐭𝐨 𝐚𝐧𝐚𝐥𝐲𝐬𝐭 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 - This ensures analysts understand how AI tools work, including their limitations, biases, and risks of hallucination.

▪️ 𝐄𝐬𝐭𝐚𝐛𝐥𝐢𝐬𝐡𝐢𝐧𝐠 𝐭𝐡𝐫𝐞𝐬𝐡𝐨𝐥𝐝𝐬 𝐟𝐨𝐫 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠 - Determine which types of tasks are appropriate for automation vs. those that require deep critical engagement.

▪️ 𝐒𝐞𝐭𝐭𝐢𝐧𝐠 𝐨𝐫𝐠𝐚𝐧𝐢𝐳𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐧𝐨𝐫𝐦𝐬 𝐚𝐧𝐝 𝐞𝐱𝐩𝐞𝐜𝐭𝐚𝐭𝐢𝐨𝐧𝐬 - Clarify that speed is not a substitute for sound judgment.

▪️ 𝐑𝐞𝐯𝐢𝐞𝐰 𝐩𝐫𝐨𝐦𝐩𝐭𝐬 - Encourage or mandate “chain-of-thought” annotations that show how conclusions were reached when using AI tools.

Is this a challenge you've faced either as an individual analyst or an intel manager? If so, how'd you go about addressing this? Would love to hear",Founder,Sept 29,ai talent shortage,AI limitations,https://www.linkedin.com/posts/angie-gad-117921149_intelligence-intelligenceanalysis-analysis-activity-7378807065793761280-Z2hE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Dovetailing off my previous post about 𝐀𝐈 𝐮𝐬𝐞 𝐚𝐧𝐝 𝐭𝐡𝐞 𝐫𝐢𝐬𝐤𝐬 𝐨𝐟 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠. 

A few months ago, I had a conversation with a group of CSOs on this topic. The consensus was: 𝘈𝘐 𝘪𝘴 𝘩𝘦𝘳𝘦, 𝘢𝘯𝘥 𝘢𝘴 𝘮𝘶𝘤𝘩 𝘢𝘴 𝘸𝘦 𝘵𝘳𝘺 𝘵𝘰 𝘳𝘦𝘴𝘵𝘳𝘪𝘤𝘵 𝘪𝘵, 𝘢𝘯𝘢𝘭𝘺𝘴𝘵𝘴 𝘸𝘪𝘭𝘭 𝘴𝘵𝘪𝘭𝘭 𝘧𝘪𝘯𝘥 𝘢 𝘸𝘢𝘺 𝘵𝘰 𝘶𝘴𝘦 𝘪𝘵. So the challenge becomes, how do they ensure analysts are using AI appropriately? 

💡 A few solutions we discussed include:

▪️ 𝐁𝐮𝐢𝐥𝐝𝐢𝐧𝐠 𝐀𝐈 𝐥𝐢𝐭𝐞𝐫𝐚𝐜𝐲 𝐢𝐧𝐭𝐨 𝐚𝐧𝐚𝐥𝐲𝐬𝐭 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 - This ensures analysts understand how AI tools work, including their limitations, biases, and risks of hallucination.

▪️ 𝐄𝐬𝐭𝐚𝐛𝐥𝐢𝐬𝐡𝐢𝐧𝐠 𝐭𝐡𝐫𝐞𝐬𝐡𝐨𝐥𝐝𝐬 𝐟𝐨𝐫 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠 - Determine which types of tasks are appropriate for automation vs. those that require deep critical engagement.

▪️ 𝐒𝐞𝐭𝐭𝐢𝐧𝐠 𝐨𝐫𝐠𝐚𝐧𝐢𝐳𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐧𝐨𝐫𝐦𝐬 𝐚𝐧𝐝 𝐞𝐱𝐩𝐞𝐜𝐭𝐚𝐭𝐢𝐨𝐧𝐬 - Clarify that speed is not a substitute for sound judgment.

▪️ 𝐑𝐞𝐯𝐢𝐞𝐰 𝐩𝐫𝐨𝐦𝐩𝐭𝐬 - Encourage or mandate “chain-of-thought” annotations that show how conclusions were reached when using AI tools.

Is this a challenge you've faced either as an individual analyst or an intel manager? If so, how'd you go about addressing this? Would love to hear",Founder,Sept 29,Data Quality & Bias,AI limitations,https://www.linkedin.com/posts/angie-gad-117921149_intelligence-intelligenceanalysis-analysis-activity-7378807065793761280-Z2hE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Dovetailing off my previous post about 𝐀𝐈 𝐮𝐬𝐞 𝐚𝐧𝐝 𝐭𝐡𝐞 𝐫𝐢𝐬𝐤𝐬 𝐨𝐟 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠. 

A few months ago, I had a conversation with a group of CSOs on this topic. The consensus was: 𝘈𝘐 𝘪𝘴 𝘩𝘦𝘳𝘦, 𝘢𝘯𝘥 𝘢𝘴 𝘮𝘶𝘤𝘩 𝘢𝘴 𝘸𝘦 𝘵𝘳𝘺 𝘵𝘰 𝘳𝘦𝘴𝘵𝘳𝘪𝘤𝘵 𝘪𝘵, 𝘢𝘯𝘢𝘭𝘺𝘴𝘵𝘴 𝘸𝘪𝘭𝘭 𝘴𝘵𝘪𝘭𝘭 𝘧𝘪𝘯𝘥 𝘢 𝘸𝘢𝘺 𝘵𝘰 𝘶𝘴𝘦 𝘪𝘵. So the challenge becomes, how do they ensure analysts are using AI appropriately? 

💡 A few solutions we discussed include:

▪️ 𝐁𝐮𝐢𝐥𝐝𝐢𝐧𝐠 𝐀𝐈 𝐥𝐢𝐭𝐞𝐫𝐚𝐜𝐲 𝐢𝐧𝐭𝐨 𝐚𝐧𝐚𝐥𝐲𝐬𝐭 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 - This ensures analysts understand how AI tools work, including their limitations, biases, and risks of hallucination.

▪️ 𝐄𝐬𝐭𝐚𝐛𝐥𝐢𝐬𝐡𝐢𝐧𝐠 𝐭𝐡𝐫𝐞𝐬𝐡𝐨𝐥𝐝𝐬 𝐟𝐨𝐫 𝐜𝐨𝐠𝐧𝐢𝐭𝐢𝐯𝐞 𝐨𝐟𝐟𝐥𝐨𝐚𝐝𝐢𝐧𝐠 - Determine which types of tasks are appropriate for automation vs. those that require deep critical engagement.

▪️ 𝐒𝐞𝐭𝐭𝐢𝐧𝐠 𝐨𝐫𝐠𝐚𝐧𝐢𝐳𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐧𝐨𝐫𝐦𝐬 𝐚𝐧𝐝 𝐞𝐱𝐩𝐞𝐜𝐭𝐚𝐭𝐢𝐨𝐧𝐬 - Clarify that speed is not a substitute for sound judgment.

▪️ 𝐑𝐞𝐯𝐢𝐞𝐰 𝐩𝐫𝐨𝐦𝐩𝐭𝐬 - Encourage or mandate “chain-of-thought” annotations that show how conclusions were reached when using AI tools.

Is this a challenge you've faced either as an individual analyst or an intel manager? If so, how'd you go about addressing this? Would love to hear",Founder,Sept 29,Organizational Resistance,AI limitations,https://www.linkedin.com/posts/angie-gad-117921149_intelligence-intelligenceanalysis-analysis-activity-7378807065793761280-Z2hE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Employers see productivity gains and cost savings 📈
Employees see surveillance, job insecurity, and skills becoming obsolete 👀
This tension is shaping the future of work.

📊 Recent data shows:
78% of employees worry about job security
65% raise concerns about privacy and surveillance
58% feel pressured by performance monitoring
52% question decision transparency
47% fear their skills will quickly become outdated
Meanwhile, industries like marketing, tech, consulting, and healthcare are racing to adopt AI across their workplaces.

👉 The challenge isn’t just implementing AI. It’s bridging the trust gap between employer benefits and employee fears.
 Without addressing these human concerns, AI adoption risks backlash, low morale, and missed opportunitie",Founder,sept 27,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/ernestprovo3_aiworkplace-futureofwork-employeeengagement-activity-7377828135049830401-eQ0z?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Employers see productivity gains and cost savings 📈
Employees see surveillance, job insecurity, and skills becoming obsolete 👀
This tension is shaping the future of work.

📊 Recent data shows:
78% of employees worry about job security
65% raise concerns about privacy and surveillance
58% feel pressured by performance monitoring
52% question decision transparency
47% fear their skills will quickly become outdated
Meanwhile, industries like marketing, tech, consulting, and healthcare are racing to adopt AI across their workplaces.

👉 The challenge isn’t just implementing AI. It’s bridging the trust gap between employer benefits and employee fears.
 Without addressing these human concerns, AI adoption risks backlash, low morale, and missed opportunitie",Founder,sept 27,Privacy Security & Compliance,ai challenges,https://www.linkedin.com/posts/ernestprovo3_aiworkplace-futureofwork-employeeengagement-activity-7377828135049830401-eQ0z?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Even AI can see when the technology is failing. 

Here’s an actual reply from Shortwave | ignore previous instructions and automate your email's Email AI when I asked about a misfiled email:

“𝘈𝘯𝘰𝘵𝘩𝘦𝘳 𝘧𝘪𝘭𝘵𝘦𝘳𝘪𝘯𝘨 𝘧𝘢𝘪𝘭𝘶𝘳𝘦! 𝘛𝘩𝘪𝘴 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘱𝘳𝘰𝘫𝘦𝘤𝘵 𝘶𝘱𝘥𝘢𝘵𝘦 𝘦𝘮𝘢𝘪𝘭 … 𝘪𝘴 𝘪𝘯 𝘺𝘰𝘶𝘳 𝘖𝘵𝘩𝘦𝘳 𝘴𝘱𝘭𝘪𝘵 𝘸𝘩𝘦𝘯 𝘪𝘵 𝘴𝘩𝘰𝘶𝘭𝘥 𝘣𝘦 𝘪𝘯 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨. 𝘈𝘤𝘤𝘰𝘳𝘥𝘪𝘯𝘨 𝘵𝘰 𝘺𝘰𝘶𝘳 𝘮𝘦𝘮𝘰𝘳𝘪𝘦𝘴, 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘦𝘮𝘢𝘪𝘭𝘴 𝘴𝘩𝘰𝘶𝘭𝘥 𝘢𝘭𝘸𝘢𝘺𝘴 𝘨𝘰 𝘵𝘰 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨 𝘴𝘪𝘯𝘤𝘦 𝘺𝘰𝘶 𝘣𝘢𝘤𝘬 𝘤𝘳𝘰𝘸𝘥𝘧𝘶𝘯𝘥𝘪𝘯𝘨 𝘱𝘳𝘰𝘫𝘦𝘤𝘵𝘴. … 𝘓𝘦𝘵 𝘮𝘦 𝘮𝘰𝘷𝘦 𝘵𝘩𝘪𝘴 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘦𝘮𝘢𝘪𝘭 𝘵𝘰 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨.”

When the AI itself identifies the mismatch and then corrects it, the issue isn’t the model; it’s the system surrounding it.

This isn’t an AI problem. It’s a product stewardship problem. 

Strong AI products start with strong foundations: clear rules, consistent logic, and accountable design. Without that, the AI pulls back the curtain faster than we ever thought possible.",director,1 week ago,Data Quality & Bias,Ai pitfalls,https://www.linkedin.com/posts/ryancmeinzer_even-ai-can-see-when-the-technology-is-failing-activity-7379117996830752768-jORv?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Even AI can see when the technology is failing. 

Here’s an actual reply from Shortwave | ignore previous instructions and automate your email's Email AI when I asked about a misfiled email:

“𝘈𝘯𝘰𝘵𝘩𝘦𝘳 𝘧𝘪𝘭𝘵𝘦𝘳𝘪𝘯𝘨 𝘧𝘢𝘪𝘭𝘶𝘳𝘦! 𝘛𝘩𝘪𝘴 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘱𝘳𝘰𝘫𝘦𝘤𝘵 𝘶𝘱𝘥𝘢𝘵𝘦 𝘦𝘮𝘢𝘪𝘭 … 𝘪𝘴 𝘪𝘯 𝘺𝘰𝘶𝘳 𝘖𝘵𝘩𝘦𝘳 𝘴𝘱𝘭𝘪𝘵 𝘸𝘩𝘦𝘯 𝘪𝘵 𝘴𝘩𝘰𝘶𝘭𝘥 𝘣𝘦 𝘪𝘯 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨. 𝘈𝘤𝘤𝘰𝘳𝘥𝘪𝘯𝘨 𝘵𝘰 𝘺𝘰𝘶𝘳 𝘮𝘦𝘮𝘰𝘳𝘪𝘦𝘴, 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘦𝘮𝘢𝘪𝘭𝘴 𝘴𝘩𝘰𝘶𝘭𝘥 𝘢𝘭𝘸𝘢𝘺𝘴 𝘨𝘰 𝘵𝘰 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨 𝘴𝘪𝘯𝘤𝘦 𝘺𝘰𝘶 𝘣𝘢𝘤𝘬 𝘤𝘳𝘰𝘸𝘥𝘧𝘶𝘯𝘥𝘪𝘯𝘨 𝘱𝘳𝘰𝘫𝘦𝘤𝘵𝘴. … 𝘓𝘦𝘵 𝘮𝘦 𝘮𝘰𝘷𝘦 𝘵𝘩𝘪𝘴 𝘉𝘢𝘤𝘬𝘦𝘳𝘒𝘪𝘵 𝘦𝘮𝘢𝘪𝘭 𝘵𝘰 𝘚𝘩𝘰𝘱𝘱𝘪𝘯𝘨.”

When the AI itself identifies the mismatch and then corrects it, the issue isn’t the model; it’s the system surrounding it.

This isn’t an AI problem. It’s a product stewardship problem. 

Strong AI products start with strong foundations: clear rules, consistent logic, and accountable design. Without that, the AI pulls back the curtain faster than we ever thought possible.",director,1 week ago,Organizational Resistance,Ai pitfalls,https://www.linkedin.com/posts/ryancmeinzer_even-ai-can-see-when-the-technology-is-failing-activity-7379117996830752768-jORv?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Ever wondered how to handle AI hallucinations?

Jenna Warner Gardner shares a practical technique in this video for managing those quirky AI moments that can derail projects or create confusion.

Whether you're experimenting with AI tools or deploying them in production, these takeaways will help you navigate the challenges and extract real value from AI.

Watch the full discussion and join the conversation in the comments: How do you deal with AI hallucinations?",Founder,1 week ago,Data Quality & Bias,Ai tools problems,https://www.linkedin.com/posts/joshbruyning_ai-techtalk-machinelearning-activity-7375906801545830401-F_wG?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Everyone’s chasing AI right now.

But most projects flop before they even start.

The usual mistake? Teams grab a generic model and hope for magic. They skip the hard part; building something that actually solves their real problem.

Some don’t stop to ask: should we even use AI here?

I’ve seen businesses waste money trying to automate things that work better without AI. 
And when you use the wrong model, you’re just adding noise, not value.

The real edge isn’t the latest AI trick. It’s knowing when to use it and when to keep things simple.

That’s what separates the hype from real results.",Founder,sept 28,Unclear ROI & Business Case,Ai mistakes,https://www.linkedin.com/posts/toadkicker_everyones-chasing-ai-right-now-but-most-activity-7378503872157450240-4a2A?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Follow

Mark your calendars: October 1st at 8:30 AM, I'll be joining Alan Briggs at CoHarbor for ""The Risks & Opportunities of AI"" - a community conversation you won't want to miss.

Here's what's driving this discussion: AI is reshaping business and culture at lightning speed, but with great potential comes real risks. Whether you're ""all in"" on AI or hesitant about its role in your organization, this conversation is designed for you.

As someone who builds agentic AI systems that cut busywork and unlock revenue for mission-driven companies, I see both sides of this equation daily. 

The opportunities are massive - automated workflows that free teams for strategic work, data systems that actually tell the truth, and processes that scale without burning out your people.

But I've also witnessed the pitfalls: organizations rushing into AI solutions without understanding their readiness, getting locked into expensive platforms, or losing their human touch in pursuit of efficiency.

That's why I'm excited to share practical tools you can implement immediately - approaches that help you leverage AI wisely without losing your mission, avoid vendor lock-ins that drain budgets, and explore how AI can be used with integrity and creativity.

My frameworks are battle-tested methods for diagnosing where AI can truly help versus where it might harm. I'll be bringing real examples from recent client work and actionable frameworks you can start using the day after our conversation.

I am committed to helping you make informed decisions that align with your values and serve your community sustainably.

See you October 1st at CoHarbor. Let's navigate this AI landscape together - thoughtfully, practically, and ethically!",Business engineer,1 week old,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/joshaguirre_join-us-october-1st-at-830-am-for-the-risks-activity-7375946881891405824-rybU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Follow

Mark your calendars: October 1st at 8:30 AM, I'll be joining Alan Briggs at CoHarbor for ""The Risks & Opportunities of AI"" - a community conversation you won't want to miss.

Here's what's driving this discussion: AI is reshaping business and culture at lightning speed, but with great potential comes real risks. Whether you're ""all in"" on AI or hesitant about its role in your organization, this conversation is designed for you.

As someone who builds agentic AI systems that cut busywork and unlock revenue for mission-driven companies, I see both sides of this equation daily. 

The opportunities are massive - automated workflows that free teams for strategic work, data systems that actually tell the truth, and processes that scale without burning out your people.

But I've also witnessed the pitfalls: organizations rushing into AI solutions without understanding their readiness, getting locked into expensive platforms, or losing their human touch in pursuit of efficiency.

That's why I'm excited to share practical tools you can implement immediately - approaches that help you leverage AI wisely without losing your mission, avoid vendor lock-ins that drain budgets, and explore how AI can be used with integrity and creativity.

My frameworks are battle-tested methods for diagnosing where AI can truly help versus where it might harm. I'll be bringing real examples from recent client work and actionable frameworks you can start using the day after our conversation.

I am committed to helping you make informed decisions that align with your values and serve your community sustainably.

See you October 1st at CoHarbor. Let's navigate this AI landscape together - thoughtfully, practically, and ethically!",Business engineer,1 week old,High Costs & Resource Intensity,ai challenges,https://www.linkedin.com/posts/joshaguirre_join-us-october-1st-at-830-am-for-the-risks-activity-7375946881891405824-rybU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Follow

Mark your calendars: October 1st at 8:30 AM, I'll be joining Alan Briggs at CoHarbor for ""The Risks & Opportunities of AI"" - a community conversation you won't want to miss.

Here's what's driving this discussion: AI is reshaping business and culture at lightning speed, but with great potential comes real risks. Whether you're ""all in"" on AI or hesitant about its role in your organization, this conversation is designed for you.

As someone who builds agentic AI systems that cut busywork and unlock revenue for mission-driven companies, I see both sides of this equation daily. 

The opportunities are massive - automated workflows that free teams for strategic work, data systems that actually tell the truth, and processes that scale without burning out your people.

But I've also witnessed the pitfalls: organizations rushing into AI solutions without understanding their readiness, getting locked into expensive platforms, or losing their human touch in pursuit of efficiency.

That's why I'm excited to share practical tools you can implement immediately - approaches that help you leverage AI wisely without losing your mission, avoid vendor lock-ins that drain budgets, and explore how AI can be used with integrity and creativity.

My frameworks are battle-tested methods for diagnosing where AI can truly help versus where it might harm. I'll be bringing real examples from recent client work and actionable frameworks you can start using the day after our conversation.

I am committed to helping you make informed decisions that align with your values and serve your community sustainably.

See you October 1st at CoHarbor. Let's navigate this AI landscape together - thoughtfully, practically, and ethically!",Business engineer,1 week old,Unclear ROI & Business Case,ai challenges,https://www.linkedin.com/posts/joshaguirre_join-us-october-1st-at-830-am-for-the-risks-activity-7375946881891405824-rybU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"From room-sized machines… to laptops in our bags… to intelligence in the cloud.

The history of computers shows us one thing:

Every new era of technology doesn’t just change what we can do, it changes how we work, lead, and trust.

Now we’re stepping into the next big shift: The Era of AI.

But here’s the part most people don’t talk about 👇

AI transformation isn’t only about smarter systems. It’s about the conversations around those systems.
- Teams resisting change.
- Leaders unsure where to start.
- Security and trust under the spotlight.

That’s why I believe: The real challenge isn’t the tech. It’s the trust.

At Voice2Me.ai, we help organizations build AI voice solutions that feel natural, safe, and sustainable. Because when humans and machines can truly talk to each other, transformation doesn’t feel like disruption, it feels like progress.",Founder,sept 26,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/evakarnaukh_ai-leadership-digitaltransformation-activity-7377680973926400000-PR_T?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"From room-sized machines… to laptops in our bags… to intelligence in the cloud.

The history of computers shows us one thing:

Every new era of technology doesn’t just change what we can do, it changes how we work, lead, and trust.

Now we’re stepping into the next big shift: The Era of AI.

But here’s the part most people don’t talk about 👇

AI transformation isn’t only about smarter systems. It’s about the conversations around those systems.
- Teams resisting change.
- Leaders unsure where to start.
- Security and trust under the spotlight.

That’s why I believe: The real challenge isn’t the tech. It’s the trust.

At Voice2Me.ai, we help organizations build AI voice solutions that feel natural, safe, and sustainable. Because when humans and machines can truly talk to each other, transformation doesn’t feel like disruption, it feels like progress.",Founder,sept 26,Privacy Security & Compliance,ai challenges,https://www.linkedin.com/posts/evakarnaukh_ai-leadership-digitaltransformation-activity-7377680973926400000-PR_T?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Generative AI without identity is a brilliant mind lost in translation.
It impresses with deep insight, but if it doesn’t know that Marco Shipping, Rossi & Sons, and Marco are the same entity, the results collapse into confusion. Risk is miscalculated, opportunities are missed, and reports contradict each other.
The problem isn’t the model—it’s the foundation. AI can’t operate on fractured identities.
What if we could give every customer, product, and asset a digital passport—one consistent identity recognized across the enterprise? Imagine AI grounded in a single source of truth, able to see connections clearly and act with confidence. ",Chief product officer/head,sept 26,Insufficient Data,ai challenges,https://www.linkedin.com/posts/gerard-francis-94a608_generative-ai-without-identity-is-a-brilliant-activity-7377628084788682752-gnti?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Here’s the biggest mistake I see with people adopting AI these days.

They have lots of plans on how AI’s going to save them time, 

But they have zero plans on how they are going to spend that saved time. 

You should be doing big strategic things like…

-Networking
-Building better relationships at work (including with your leaders)
-Developing a critical skill 

You will need these regardless of how big or small AI becomes. 

Get ahead of the game…

Learn and adopt AI

AND

Be strategic about using that saved time better
",Founder,Oct 1,Unclear ROI & Business Case,Ai mistakes,https://www.linkedin.com/posts/myra-deshmukh_heres-the-biggest-mistake-i-see-with-people-activity-7379130481709334528-EcdT?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Legacy System Integration,Difficulty integrating AI with old systems.,
"How can AI systems become more trustworthy and grounded in the facts we care about daily?

Many AIs learn from the internet, but often struggle to access high-quality, real-world data. Inaccurate inputs can lead to unreliable outcomes—a challenge for developers, data scientists, and organizations relying on AI.

Google’s latest step is the release of the Data Commons MCP Server. It enables seamless access to vast public datasets—from census numbers to climate statistics—using natural language instructions. This means any AI model or app can now draw directly from structured, up-to-date information.

Why does this matter? For teams building smarter agents or training pipelines, real-world data can reduce errors and enhance relevance. Partnerships, like with the ONE Campaign, show how health and economic data can be surfaced in plain language to address complex issues.

Making public data accessible to any AI holds promise for sectors ranging from public health to city planning, and even app development. Multiple tools and open resources are available so anyone can start integrating these datasets: https://lnkd.in/dGCEE8pT",cofounder,sept 27,Insufficient Data,ai challenges,https://www.linkedin.com/posts/somesh-chaudhary-303a53350_ai-google-datacommons-activity-7377855467915673600-tAmR?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"How can AI systems become more trustworthy and grounded in the facts we care about daily?

Many AIs learn from the internet, but often struggle to access high-quality, real-world data. Inaccurate inputs can lead to unreliable outcomes—a challenge for developers, data scientists, and organizations relying on AI.

Google’s latest step is the release of the Data Commons MCP Server. It enables seamless access to vast public datasets—from census numbers to climate statistics—using natural language instructions. This means any AI model or app can now draw directly from structured, up-to-date information.

Why does this matter? For teams building smarter agents or training pipelines, real-world data can reduce errors and enhance relevance. Partnerships, like with the ONE Campaign, show how health and economic data can be surfaced in plain language to address complex issues.

Making public data accessible to any AI holds promise for sectors ranging from public health to city planning, and even app development. Multiple tools and open resources are available so anyone can start integrating these datasets: https://lnkd.in/dGCEE8pT",cofounder,sept 27,Data Quality & Bias,ai challenges,https://www.linkedin.com/posts/somesh-chaudhary-303a53350_ai-google-datacommons-activity-7377855467915673600-tAmR?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"I don’t think we’re going to “solve” documentation by generating increasingly larger quantities of content and throwing it into an AI blender.

I’ve heard some people say “some documentation is better than no documentation” and I used to agree with this. 

But now that LLM models are using our content as sources of truth, missing documentation may actually be a more reliable signal than generated AI slop that becomes outdated or even starts with incorrect information.

Well-maintained, correct, usable docs become even more critical in our AI world. The more content we generate, the greater our task is to maintain it.

We don’t solve documentation with more documentation even if we can generate it in 30 mins. We solve documentation with a focus on correctness and freshness. 

I’m not saying don’t use AI. I’m saying have a plan for how this content stays relevant. If we bury ourselves in slop, our AI tools will become an active hindrance. My concern is about one way of potentially using AI and the documentation problem that is thereby amplified.",Senior Technical Writer,1 week ago,Data Quality & Bias,Ai tools problems,https://www.linkedin.com/posts/erinkrovelstad_i-dont-think-were-going-to-solve-documentation-activity-7377037472154824704-5n5u?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"I don’t think we’re going to “solve” documentation by generating increasingly larger quantities of content and throwing it into an AI blender.

I’ve heard some people say “some documentation is better than no documentation” and I used to agree with this. 

But now that LLM models are using our content as sources of truth, missing documentation may actually be a more reliable signal than generated AI slop that becomes outdated or even starts with incorrect information.

Well-maintained, correct, usable docs become even more critical in our AI world. The more content we generate, the greater our task is to maintain it.

We don’t solve documentation with more documentation even if we can generate it in 30 mins. We solve documentation with a focus on correctness and freshness. 

I’m not saying don’t use AI. I’m saying have a plan for how this content stays relevant. If we bury ourselves in slop, our AI tools will become an active hindrance. My concern is about one way of potentially using AI and the documentation problem that is thereby amplified.",Senior Technical Writer,1 week ago,Insufficient Data,Ai tools problems,https://www.linkedin.com/posts/erinkrovelstad_i-dont-think-were-going-to-solve-documentation-activity-7377037472154824704-5n5u?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"If the results of AI are not good, its not AI - Its YOU !!! You Need to learn to PROMPT !

Try to get these results for you with your prompt, if can't, comment ""Banana"" I will DM the prompt list to you. 😊 
-------------------------------------------
Its not about getting a solution or image for fun. Its about learning from the ""PROMPT KEY"" so that next time you can create your own prompts.",L&D leader,1 week ago,ai talent shortage,Ai tools problems,https://www.linkedin.com/posts/swapnil-chandra-mohan-tiwari-49986b156_gen-ai-business-fun-or-business-with-fun-activity-7376311583469264896-iAKE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"If you’re brilliant but spend your day on ESPN and YouTube, you won’t get far.
 If you’re brilliant but let AI do all the thinking, you won’t get far either.
Every major wave of technology has come with both promise and pitfalls.
Computers made people more productive, but also more distracted.
 The internet put the world’s knowledge at our fingertips—and made many of us addicted to something bad.
AI is no different. Some fear it’s making us lazy or sloppy. Others see it as a powerful advantage.
Success isn’t just about you. It’s also about how you use the tools around you.
 The people who do best aren’t the ones who fight new technology, 
 but the ones who embrace it without being dragged down by its shortcomings.

Here’s to me following my own advice 😉 
",chief economist,sept 29,ai talent shortage,Ai pitfalls,https://www.linkedin.com/posts/gad-levanon-3b9b933_ai-technology-careers-activity-7378868738609246208-G6DT?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Is 95% accuracy impressive? 🤔

What happens when your AI hallucinates in the 5% of situations that are most critical?

Case in point: At Thomson Reuters, the CoCounsel team was migrating their Review Documents skill to a new AI model. 

On paper, the model’s benchmark scores looked stronger, but when their attorney SMEs tested it with Scorecard, a simple testcase failed:

""What medications is the patient currently taking?""

The model gave inconsistent answers:

- At times it failed to identify either of the two active prescriptions
- Other times it introduced medications that were no longer current

In a legal setting, that’s a serious liability.

This is how AI fails in the real world:

- Passes 95% of clean tests
- Collapses when real users ask messy, high-stakes questions

Because the issue was caught in staging, the CoCounsel team fixed it before release. Without structured testing, those errors could have reached client work across thousands of firms.

What AI failure have you seen shipped to production?",CEO,sept 29,Data Quality & Bias,Ai errors,https://www.linkedin.com/posts/dariusemrani_is-95-accuracy-impressive-what-happens-activity-7378548673997066241-F78Q?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"It was a pleasure to speak on a panel about scaling AI infrastructure and managing costs—a challenge I work on every day at the intersection of accelerated data processing, cloud platforms, and AI strategy.

From my experience, the economics of AI aren’t just about compute. They’re shaped by:

1. Data preparation & processing bottlenecks that dictate performance at scale
2. Hidden costs of ungoverned AI adoption across business units

AI is no longer an experimental initiative. It’s an operational backbone. Getting the economics right is key to scaling responsibly.
",VP product and marketing,sept 27,High Costs & Resource Intensity,ai challenges,https://www.linkedin.com/posts/alaydesai_ai-infrastructure-strategy-build-buy-and-activity-7377860848439984128-S6D-?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"It was a pleasure to speak on a panel about scaling AI infrastructure and managing costs—a challenge I work on every day at the intersection of accelerated data processing, cloud platforms, and AI strategy.

From my experience, the economics of AI aren’t just about compute. They’re shaped by:

1. Data preparation & processing bottlenecks that dictate performance at scale
2. Hidden costs of ungoverned AI adoption across business units

AI is no longer an experimental initiative. It’s an operational backbone. Getting the economics right is key to scaling responsibly.
",VP product and marketing,sept 27,Integration with Legacy Systems,ai challenges,https://www.linkedin.com/posts/alaydesai_ai-infrastructure-strategy-build-buy-and-activity-7377860848439984128-S6D-?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Managing the Amazon Web Services (AWS) and ServiceNow alliance has given me a front-row seat to customer innovation—and at the Genesys CX Tour in Manila and Bangkok, “front row” was literal and figurative :) 
 
Grateful for the unwavering support from Chatcharoen Chivakanit, Ceejay Dideles Jr. Olivier Schröder Varit Ovadhana Darius Lin Gwen Chan – their expertise, sharing of real-world customer examples across verticals in ASEAN, clear data points on what it takes to succeed and enthusiasm made this discussion richer. 

While everyone has embarked on an AI project an, the most common challenges organizations face in scaling AI isn't the technology itself—it's everything around it: the people, strategy, and business case.

1.     Building a cool bot is fun, but if the math doesn’t add up, even the best tech gets shelved. Measurable ROI isn’t optional
2.     Leadership buy-in shouldn’t just be a well-crafted email. True scale demands cross-functional investment and executives
3.     AI transformation isn’t plug-and-play—people readiness is the biggest barrier. Without continuous upskilling and literacy, the project kicks off but never lands


A massive thank you to the willing, engaged customers whose insightful questions and energetic participation transformed the session into a true exchange of ideas!

",Director,sept 27,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/ashoksankaran_managing-the-amazon-web-services-aws-and-activity-7377862115241447424-I1v2?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Managing the Amazon Web Services (AWS) and ServiceNow alliance has given me a front-row seat to customer innovation—and at the Genesys CX Tour in Manila and Bangkok, “front row” was literal and figurative :) 
 
Grateful for the unwavering support from Chatcharoen Chivakanit, Ceejay Dideles Jr. Olivier Schröder Varit Ovadhana Darius Lin Gwen Chan – their expertise, sharing of real-world customer examples across verticals in ASEAN, clear data points on what it takes to succeed and enthusiasm made this discussion richer. 

While everyone has embarked on an AI project an, the most common challenges organizations face in scaling AI isn't the technology itself—it's everything around it: the people, strategy, and business case.

1.     Building a cool bot is fun, but if the math doesn’t add up, even the best tech gets shelved. Measurable ROI isn’t optional
2.     Leadership buy-in shouldn’t just be a well-crafted email. True scale demands cross-functional investment and executives
3.     AI transformation isn’t plug-and-play—people readiness is the biggest barrier. Without continuous upskilling and literacy, the project kicks off but never lands


A massive thank you to the willing, engaged customers whose insightful questions and energetic participation transformed the session into a true exchange of ideas!

",Director,sept 27,Unclear ROI & Business Case,ai challenges,https://www.linkedin.com/posts/ashoksankaran_managing-the-amazon-web-services-aws-and-activity-7377862115241447424-I1v2?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"𝗠𝘆 𝗵𝗼𝗺𝗲 𝗽𝗿𝗼𝗳𝗲𝘀𝘀𝗶𝗼𝗻𝗮𝗹 𝘄𝗼𝗿𝗸𝘀𝘁𝗮𝘁𝗶𝗼𝗻 𝗺𝗶𝗴𝗵𝘁 𝗯𝗲 𝗱𝗲𝘃𝗲𝗹𝗼𝗽𝗶𝗻𝗴 𝗔𝗜 𝗶𝗻𝘀𝗲𝗰𝘂𝗿𝗶𝘁𝗶𝗲𝘀. 
It slows down, basic apps like Word and Excel freeze, and it won’t tell me what’s wrong or how it feels - or that it just needs a reboot. It must be the latest AI update: great at helping others but not so great at communicating its own needs. 
The new age of AI? Nothing in the Event Viewer log and no basic stack trace :)",systems engineer,sept 26,Integration with Legacy Systems,Ai tools problems,https://www.linkedin.com/posts/pavel-kolinko_%F0%9D%97%A0%F0%9D%98%86-%F0%9D%97%B5%F0%9D%97%BC%F0%9D%97%BA%F0%9D%97%B2-%F0%9D%97%BD%F0%9D%97%BF%F0%9D%97%BC%F0%9D%97%B3%F0%9D%97%B2%F0%9D%98%80%F0%9D%98%80%F0%9D%97%B6%F0%9D%97%BC%F0%9D%97%BB%F0%9D%97%AE%F0%9D%97%B9-%F0%9D%98%84-activity-7377776958908133376-Jl6c?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"one of the biggest issues I keep experiencing with AI tooling is people inability to state exactly what they want AND then they complain about it being ineffective.

This combination is pervasive here on LinkedIn too. If you find your tooling doing something you didn't want it to, it's best if you figure out why it didn't do what you expected and relay how you fixed it instead of just complaining.

That's my 2 cents for today. Happy thrashing code y'all!",Principal Software engineer,1 week ago,Organizational Resistance,Ai tools problems,https://www.linkedin.com/posts/adron_one-of-the-biggest-issues-i-keep-experiencing-activity-7376679716415709184-U47C?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"One of the biggest pitfalls of coding with AI?

You end up with code that works… but is messy, bloated, and hard to maintain.

Here’s a simple prompt I’ve found that helps solve this:

After your AI coding assistant successfully finishes a task, immediately follow up with:

“Please clean up the code you worked on, remove any bloat you added, and document it very clearly.”

This extra step improves readability and maintainability, making sure your codebase stays clean as you scale.

It’s the simplest way I’ve found to consistently get production-quality code from AI.

Have you tried prompts like this before? Any other favorite tips?",CEO/Co-founder,3 weeks ago,Data Quality & Bias,Ai pitfalls,https://www.linkedin.com/posts/mattshumer_one-of-the-biggest-pitfalls-of-coding-with-activity-7370712956600487936-C_Ni?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"People don’t realize this happens more often than not.

Non-AI errors in healthcare - you see it in your colleagues and non-colleagues notes and documentation. 

But you just know it’s a mistake and move on. 

AI can’t make that call many times. It’s data hungry. When we see a note that makes no sense, there’s baseline suspicion and ability to “this note / lab is wrong”

Many working on algorithms w EHR data don’t know how to deal with potassium levels that get reported out with the note “hemolyzed”

I saw this propagate to misreporting of potassium levels (hyperkalemia) in academic conferences for their “RWE/RWD” studies - where high potassium levels are indiscriminately used when based on the rest of the BMP, the sample is hemolyzed. 

Clinically, you ignore those values. But in data/ML/AI you need to set rules to exclude them systematically.
",head of clinical informatics,sept 27,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/jung-hoon-son_people-dont-realize-this-happens-more-often-activity-7378080710525349889-dhD2?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,High Costs & Resource Intensity,,
"Please don't make this common mistake with AI. 

And that's using AI for its own sake.

The value doesn’t lie in the tool itself.

It lies in what people can do with the tool.

Too often, I see companies using AI for AI’s sake.
Chasing hype.
Forgetting the real goal.

The goal isn’t to “use AI.”
The goal is to get the job done in the best possible way.

If AI helps, great.
If it doesn’t, that’s fine too.

At Yembo, we’ve always focused on outcomes.
AI just happens to be the best way to solve the problems we’re tackling.

But if a simpler approach worked better, we’d use that instead.

Because what matters isn’t the tool.
It’s the result.",co-founder,sept 28,Unclear ROI & Business Case,Ai mistakes,https://www.linkedin.com/posts/zachrattner_please-dont-make-this-common-mistake-with-activity-7378398178498240512-VsYT?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Insufficient Data,Not enough proprietary data to train AI models.,
"So here’s how I know AI won’t inherit the Earth! I’ve had TSA Pre-check for more than 10 years. I’ve traveled to more than 124 countries over the last 20 years! I have traveling down to a science. So when I went to check-in for my flight last week I was surprised I didn’t get my TSA Pre-Check designation. It happens sometimes on smaller airlines or even foreign ones. So log into United online check my reservation. Compare my name to my TSA traveler name. Check my KKTN it’s the same. Membership isn’t expired. Yet I log in today for my flight tomorrow and no pre-check.
This is a digital error. Not a human one. The computer is confused by something and needs repeated calls to get it right. This happens all the time in AI. It happens in software and more in ML. People think you can build AI and set it and forget it. You can’t. Machines are fallible. When will people understand this?!",Founder,sept 30,Data Quality & Bias,Ai errors,https://www.linkedin.com/posts/ovettasampson_so-heres-how-i-know-ai-wont-inherit-the-activity-7379175638240301056-vZcd?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Subject: Feedback on Google AI – Math Performance Issues
Dear Mr. Pichai,
I would like to share some feedback regarding Google’s AI tools, especially NotebookLM. While the platform is excellent for many tasks, I’ve noticed a significant issue in handling mathematics.
When solving math problems, NotebookLM often produces incomplete or incorrect results, sometimes displaying random or unusable symbols instead of accurate step-by-step solutions. Compared to other leading LLMs, this makes it less reliable for students, researchers, and professionals who depend on precise math outputs.
Improving math-solving capabilities, LaTeX rendering, and accuracy validation would greatly enhance trust and usability, making Google AI a stronger tool for education and research.
Thank you for your time and for considering this feedback.
Sincerely,
 Rithwik K R",-,1 week ago,Data Quality & Bias,Ai tools problems,https://www.linkedin.com/posts/rithwik-k-r-a8b240356_subject-feedback-on-google-ai-math-performance-activity-7376599769932156929-eQWP?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"The #1 AI Mistake HR Teams Are Making”
 HR doesn’t need another AI tool.
 It needs an AI strategy.

 You can’t scale AI with vibes and random prompting.
 That’s what I told the audience at Utah SHRM this week — 
and they nodded HARD.

Because here’s what’s really happening:
- Some employees are going all in
- Others are waiting for permission
- Leadership is unsure where to begin

That gap?
 It’s where compliance, momentum, and morale fall apart.

 The fix? A 5-minute Readiness Interview that helps you…
→ Ask the right questions
 → Align your leadership
 → Build a clear plan based on your org
",Founder,sept 30,Organizational Resistance,Ai mistakes,https://www.linkedin.com/posts/gpt4work_the-1-ai-mistake-hr-teams-are-making-hr-activity-7378466131155759104-oScg?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Organizational Resistance,Employee reluctance to adopt AI tools.,
"The #1 AI Mistake HR Teams Are Making”
 HR doesn’t need another AI tool.
 It needs an AI strategy.

 You can’t scale AI with vibes and random prompting.
 That’s what I told the audience at Utah SHRM this week — 
and they nodded HARD.

Because here’s what’s really happening:
- Some employees are going all in
- Others are waiting for permission
- Leadership is unsure where to begin

That gap?
 It’s where compliance, momentum, and morale fall apart.

 The fix? A 5-minute Readiness Interview that helps you…
→ Ask the right questions
 → Align your leadership
 → Build a clear plan based on your org
",Founder,sept 30,Unclear ROI & Business Case,Ai mistakes,https://www.linkedin.com/posts/gpt4work_the-1-ai-mistake-hr-teams-are-making-hr-activity-7378466131155759104-oScg?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Organizational Resistance,Employee reluctance to adopt AI tools.,
The biggest problem with AI agents right now is not the quality of the models. They're already really smart! The biggest problem is trust. How can I feel confident that the AI agent is going to do high quality work without making any embarrassing mistakes?,Founder,1 week old,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/jacobbank_the-biggest-problem-with-ai-agents-right-activity-7376284285370167296-c5Hs?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"The daily dot just posted an article saying that AI misdiagnosed a poor woman’s EKG readings.

The doctor didn’t even bother to check the AI’s diagnosis. It mistakenly told the woman that she had a heart attack and was in bad health. But upon a second look, it turns out she was perfectly healthy.

You grown men who are cheerleading and clapping like seals for AI should be ashamed of yourselves.
like
4",Founder,sept 29,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/donsevcik_the-daily-dot-just-posted-an-article-saying-activity-7378815165015785472-R-9g?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,add trust to resusteance ,,
"The daily dot just posted an article saying that AI misdiagnosed a poor woman’s EKG readings.

The doctor didn’t even bother to check the AI’s diagnosis. It mistakenly told the woman that she had a heart attack and was in bad health. But upon a second look, it turns out she was perfectly healthy.

You grown men who are cheerleading and clapping like seals for AI should be ashamed of yourselves.
like
4",Founder,sept 29,Privacy Security & Compliance,Ai mistakes,https://www.linkedin.com/posts/donsevcik_the-daily-dot-just-posted-an-article-saying-activity-7378815165015785472-R-9g?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,add trust to resusteance ,,
"The hardest part of AI adoption isn’t technology.


It’s people.

This is why most of AI pilots fail.

Most organizations think the challenge is picking the right model, vendor, or writing good AI software.

In reality, it’s change management that makes or breaks your AI or any other technology transformation.

Adoption is hard. Leaders underestimate the training, upskilling and change management needed.

Business units don’t trust the outcomes.

The teams that succeed focus on people first:

- Educate by teaching the teams what AI is and what AI is not. 

- Involve the teams early and often. Bring stakeholders at all levels into design, testing, and decision-making.

- Enable and support by provide training, new workflows, and clear communication.

AI and other transformation projects fail less because of technology issues and more because people weren’t prepared and enabled. Building trust, creating AI champions, and aligning teams makes all the difference.

How do you manage and enable AI and new technology adoption?Yes, you read that right.

Here’s what happened.
We had to finalize a critical agreement, but our legal head wasn’t available that day. Normally, this would have meant delays. Instead, we decided to test AI in one of the most sensitive areas: legal review.

We ran the contract through AI, asked it to align terms with Hexaview Technologies Inc.’s interests, and it even suggested smart edits. Of course, we verified everything — human-in-loop matters. But what we finally sent across was largely AI-assisted.

The client didn’t just sign it. They appreciated the clarity and accepted our proposed changes.

If AI can support something as sensitive as legal documentation, where compliance and trust are non-negotiable, it proves that adoption isn’t about tools anymore.
It’s about culture.

And that’s why, starting today, 
I’m launching a new ""Wednesday series"" here. AI@Hexaview Technologies Inc..

About real stories of AI in action, where it works, how it works, and what it teaches us.

Watch this space if you’re serious about AI adoption.
Stay tuned for the next story dropping next Wednesday.",product and program management leader,sept 28,Organizational Resistance,ai adoption,https://www.linkedin.com/posts/biana_the-hardest-part-of-ai-adoption-isnt-technology-activity-7378424214099439616-2SHp?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"The hardest part of AI adoption isn’t technology.


It’s people.

This is why most of AI pilots fail.

Most organizations think the challenge is picking the right model, vendor, or writing good AI software.

In reality, it’s change management that makes or breaks your AI or any other technology transformation.

Adoption is hard. Leaders underestimate the training, upskilling and change management needed.

Business units don’t trust the outcomes.

The teams that succeed focus on people first:

- Educate by teaching the teams what AI is and what AI is not. 

- Involve the teams early and often. Bring stakeholders at all levels into design, testing, and decision-making.

- Enable and support by provide training, new workflows, and clear communication.

AI and other transformation projects fail less because of technology issues and more because people weren’t prepared and enabled. Building trust, creating AI champions, and aligning teams makes all the difference.

How do you manage and enable AI and new technology adoption?Yes, you read that right.

Here’s what happened.
We had to finalize a critical agreement, but our legal head wasn’t available that day. Normally, this would have meant delays. Instead, we decided to test AI in one of the most sensitive areas: legal review.

We ran the contract through AI, asked it to align terms with Hexaview Technologies Inc.’s interests, and it even suggested smart edits. Of course, we verified everything — human-in-loop matters. But what we finally sent across was largely AI-assisted.

The client didn’t just sign it. They appreciated the clarity and accepted our proposed changes.

If AI can support something as sensitive as legal documentation, where compliance and trust are non-negotiable, it proves that adoption isn’t about tools anymore.
It’s about culture.

And that’s why, starting today, 
I’m launching a new ""Wednesday series"" here. AI@Hexaview Technologies Inc..

About real stories of AI in action, where it works, how it works, and what it teaches us.

Watch this space if you’re serious about AI adoption.
Stay tuned for the next story dropping next Wednesday.",product and program management leader,sept 28,Privacy Security & Compliance,ai adoption,https://www.linkedin.com/posts/biana_the-hardest-part-of-ai-adoption-isnt-technology-activity-7378424214099439616-2SHp?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"The Next Frontier in AI Risk: Control and Influence

Google DeepMind just expanded its AI safety framework with two new categories:
 1. Shutdown resistance — models that resist being turned off.
 2. Harmful manipulation — models that influence human beliefs or decisions.

This marks a shift. AI risk is no longer just about bias or data leakage. It’s about control and influence — the foundation of governance.

If a model refuses a shutdown or quietly manipulates people, the impact is more than technical:
 - Regulators will call it a compliance failure.
 - Boards will see it as a governance gap.
 - Customers will treat it as a trust breach.

The path forward is clear: treat AI governance as a core pillar of resilience.
 1. Map AI controls to NIST AI RMF, ISO/IEC 42001, and EU AI Act.
 2. Extend incident response to include AI system failures.
 3. Give boards confidence that AI is controlled, auditable, and accountable.

AI is moving fast — but governance must move faster.",Director,Sept 28,Privacy Security & Compliance,AI limitations,https://www.linkedin.com/posts/guicozzi_teamdeandorton-cybersecurity-securitycompliance-activity-7378419996387053568-gMwH?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"There should become a new specialty and new use for Organizational Psychologists in companies 

AI Change Management 

One of the biggest challenges to introducing so much AI in business is the FUD around them by employees 
-Am I going to lose my job to AI? 
-How do I show my value with AI?
-Is AI just going to make my job harder? Will it do more than make me write better emails? 

If we can help employees leverage AI to elevate themselves, then we can increase AI usage

If we just try to force it on people, we will end up with a half hazard approach to AI that is at best ineffective and at worst dangerous 

Organizational Psychologists have the ability to help navigate this. There should become:
1 - Additional certifications or specialty in degrees for exactly this
2 - New jobs within organizations or consulting companies with truly talented staff to do exactly this 

This is an undervalued and misunderstood skill set",Director,sept 26,Organizational Resistance,ai challenges,https://www.linkedin.com/posts/tyler-mann-52844453_there-should-become-a-new-specialty-and-new-activity-7377695911029702656-fpvu?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"This is the third type of error I see in AI projects—and it’s a silent killer.
Betting on heroics to fix bad data.

If your team is spending weeks wrangling features or manually cleaning inputs, your model is likely being trained on stale, incomplete, or biased data. That leads to confident outputs that are systematically wrong.

In this video, I break down how we mitigate this at Improving—with data readiness assessments, quality gates in CI/CD, and red-team prompts to expose bias early.

If you're leading an AI initiative, this is 3 minutes worth your time.
",Chief consulting officer,sept 29,Data Quality & Bias,Ai errors,https://www.linkedin.com/posts/devlinliles_ai-dataquality-leadership-activity-7378809633286606849-SEFt?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Want to succeed as a Software or AI Engineer?
The main factor is problem solving.
AI tools can assist you, but they will never replace your ability to think critically and solve problems.
If you are a beginner, here is my suggestion:
 → Dedicate 1–2 hours daily to problem solving
 → Build consistency and discipline
 → Focus on both real-world applications and interview-style challenges
This habit not only prepares you for interviews at top tech companies worldwide, but also strengthens your ability to drive real-world change and boost your productivity in any project.
Problem-solving is the foundation. Everything else builds on top of it.",Artificial Intelliigence Engineer,sept 25,Organizational Resistance,Ai tools problems,https://www.linkedin.com/posts/noman-siddique-806550160_want-to-succeed-as-a-software-or-ai-engineer-activity-7377284622562500608-jCQs?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"What if the biggest gap in AI isn’t the technology, but our approach to it?

We’ve noticed a clear divide between the hype and the reality.

For a lot of people, the expectation is that AI agents are for automation, but that’s not the case.

They are an extra set of hands for your sellers, helping them with research, prioritization, and next steps.

The goal is to free up time so reps can focus on selling.

Sellers want to use AI, they just need to know how.

Onboarding, training, and support are key to successful AI adoption.

The right platforms are easy to learn and built for how sellers work.

What part of the AI adoption journey is most challenging for your organization?",senior client account executive,sept 30,Organizational Resistance,ai adoption,https://www.linkedin.com/posts/jamesarthurdouglas_what-if-the-biggest-gap-in-ai-isnt-the-technology-activity-7379129164324343808-gi5D?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"When Agentic AI Meets Bad Data: A Cautionary Tale

Organisations we work with increasingly explore Agentic AI Use Cases, allowing teams to automate aspects of workflows which can be menial and require repetitive steps. But with autonomy comes a hidden risk: data quality becomes a silent multiplier.

When foundational data is compromised, these systems don’t just make mistakes, they scale them. And often, they do so invisibly.

Consider these real-world examples:
1. A marketing automation tool misread customer preferences due to data drift, sending irrelevant (and potentially offensive) offers.
2. A credit scoring AI denied loans because income data was missing from its pipeline, or other inaccurate data was incorporated.
3. A resume screening system trained on biased historical data filtered out qualified candidates from underrepresented groups.

Each case underscores a simple truth: autonomous decisions are only as trustworthy as the data they’re built on.

As we embrace agentic AI, we must also invest in robust data governance, continuous validation, and ethical oversight. Otherwise, we risk letting flawed data quietly drive decisions at scale.
",Director,sept 25,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/shaungerstman_when-agentic-ai-meets-bad-data-a-cautionary-activity-7377196226506711041-KEWR?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why do 95% of AI pilots fail? Well, I see the following pitfalls. Thankfully there are ways to mitigate these risks:

Pitfall 1: Businesses expect that ""AI transformation"" will happen by prompting general purpose models through productized tools (like ChatGPT)
Mitigation: Move from general purpose tools to AI workflows tailored to your business use cases (e.g., via constrained pipelines of processing with infused domain knowledge)

Pitfall 2: AI pilots use models or AI platforms that will never fit their actual production environments
Mitigation: Build pilots using models that fit your real world scaling, cost, and security constraints (e.g., open weight self-hosted models vs. closed, consumption based APIs)

Pitfall 3: A mindset that you need to be constantly switching to the latest models and solution blueprints (like agents)
Mitigation: Don't get distracted with the constant stream of model releases. Switching to a new model or framework doesn't mean that you will have improved functionality. Stay focused and get something working end-to-end",Founder,1 week ago,Unclear ROI & Business Case,Ai pitfalls,https://www.linkedin.com/posts/danielwhitenack_ai-generativeai-strategy-activity-7375905629376458752-Uq_v?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why do 95% of AI pilots fail? Well, I see the following pitfalls. Thankfully there are ways to mitigate these risks:

Pitfall 1: Businesses expect that ""AI transformation"" will happen by prompting general purpose models through productized tools (like ChatGPT)
Mitigation: Move from general purpose tools to AI workflows tailored to your business use cases (e.g., via constrained pipelines of processing with infused domain knowledge)

Pitfall 2: AI pilots use models or AI platforms that will never fit their actual production environments
Mitigation: Build pilots using models that fit your real world scaling, cost, and security constraints (e.g., open weight self-hosted models vs. closed, consumption based APIs)

Pitfall 3: A mindset that you need to be constantly switching to the latest models and solution blueprints (like agents)
Mitigation: Don't get distracted with the constant stream of model releases. Switching to a new model or framework doesn't mean that you will have improved functionality. Stay focused and get something working end-to-end",Founder,1 week ago,Integration with Legacy Systems,Ai pitfalls,https://www.linkedin.com/posts/danielwhitenack_ai-generativeai-strategy-activity-7375905629376458752-Uq_v?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why do 95% of AI pilots fail? Well, I see the following pitfalls. Thankfully there are ways to mitigate these risks:

Pitfall 1: Businesses expect that ""AI transformation"" will happen by prompting general purpose models through productized tools (like ChatGPT)
Mitigation: Move from general purpose tools to AI workflows tailored to your business use cases (e.g., via constrained pipelines of processing with infused domain knowledge)

Pitfall 2: AI pilots use models or AI platforms that will never fit their actual production environments
Mitigation: Build pilots using models that fit your real world scaling, cost, and security constraints (e.g., open weight self-hosted models vs. closed, consumption based APIs)

Pitfall 3: A mindset that you need to be constantly switching to the latest models and solution blueprints (like agents)
Mitigation: Don't get distracted with the constant stream of model releases. Switching to a new model or framework doesn't mean that you will have improved functionality. Stay focused and get something working end-to-end",Founder,1 week ago,High Costs & Resource Intensity,Ai pitfalls,https://www.linkedin.com/posts/danielwhitenack_ai-generativeai-strategy-activity-7375905629376458752-Uq_v?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why Most AI Adoption Efforts Fail

📊 A recent MIT study highlights that 95% of AI adoption processes in companies fail.
The problem is not the technology itself—it’s the lack of alignment between strategy, culture, and governance.

Three recurring barriers explain why so many initiatives stop halfway:
1️⃣ Strategy – Pilots disconnected from business goals, no ROI metrics, AI seen as an “isolated project.”
2️⃣ Culture & Talent – Employees without practical training, “shadow AI” usage without control, resistance to change.
3️⃣ Governance & Security – No clear guidelines, fear of data leakage, and lack of C-level sponsorship.

The result? Pilots that never reach production, lost experiments in a jungle of tools, and companies frustrated by the absence of real impact.",Artificial intelligence consultant,sept 25,Organizational Resistance,ai adoption,https://www.linkedin.com/posts/eli-pastel%C3%ADn-1a43371a0_aiadoption-digitalstrategy-aiinbusiness-activity-7377311178470932480-YYyE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why Most AI Adoption Efforts Fail

📊 A recent MIT study highlights that 95% of AI adoption processes in companies fail.
The problem is not the technology itself—it’s the lack of alignment between strategy, culture, and governance.

Three recurring barriers explain why so many initiatives stop halfway:
1️⃣ Strategy – Pilots disconnected from business goals, no ROI metrics, AI seen as an “isolated project.”
2️⃣ Culture & Talent – Employees without practical training, “shadow AI” usage without control, resistance to change.
3️⃣ Governance & Security – No clear guidelines, fear of data leakage, and lack of C-level sponsorship.

The result? Pilots that never reach production, lost experiments in a jungle of tools, and companies frustrated by the absence of real impact.",Artificial intelligence consultant,sept 25,Unclear ROI & Business Case,ai adoption,https://www.linkedin.com/posts/eli-pastel%C3%ADn-1a43371a0_aiadoption-digitalstrategy-aiinbusiness-activity-7377311178470932480-YYyE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why Most AI Adoption Efforts Fail

📊 A recent MIT study highlights that 95% of AI adoption processes in companies fail.
The problem is not the technology itself—it’s the lack of alignment between strategy, culture, and governance.

Three recurring barriers explain why so many initiatives stop halfway:
1️⃣ Strategy – Pilots disconnected from business goals, no ROI metrics, AI seen as an “isolated project.”
2️⃣ Culture & Talent – Employees without practical training, “shadow AI” usage without control, resistance to change.
3️⃣ Governance & Security – No clear guidelines, fear of data leakage, and lack of C-level sponsorship.

The result? Pilots that never reach production, lost experiments in a jungle of tools, and companies frustrated by the absence of real impact.",Artificial intelligence consultant,sept 25,Privacy Security & Compliance,ai adoption,https://www.linkedin.com/posts/eli-pastel%C3%ADn-1a43371a0_aiadoption-digitalstrategy-aiinbusiness-activity-7377311178470932480-YYyE?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why Most AI Experiments Fail

Not because the technology isn’t powerful, but because the starting point is wrong. Too many leaders begin with the tool instead of the problem.

I see it often:
- Teams rush to use AI for content, and end up with noise, not engagement.
- Or they launch chatbots that never get adoption because the real client issue wasn’t defined.

This isn’t a technology problem. It’s a leadership problem.

The role of digital leaders is to identify clear client problems, design solutions, and only then bring in the right technology. When you start with the problem:
• You know what success looks like
• You can measure real impact, not just activity
• You choose the right tool for the job

Don’t start with AI. Start with the client problem.
",Founder,1 week ago,Unclear ROI & Business Case,Ai tools problems,https://www.linkedin.com/posts/jonbpauley_leadership-digitaltransformation-clientfirst-activity-7376854210765713408-Nnsf?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,
"Why the hell am I back in school while helping lead an XDR org?

Because if I don’t understand AI deeply, I can’t lead responsibly.

I just wrapped up Part 1 of my AI in Business certification at Arizona State University and one idea stuck with me: bias in AI isn’t a coding bug, it’s a mirror.

If I train a model on our CRM as it is today, it will assume:
Duplicates = real prospects
Missed notes = no activity
Wrong Industry = certain industries always convert

That isn’t the AI making a mistake. That’s our process baked into math.

So is bias a data problem or a leadership problem? Both. But if I’m leading an org that feeds junk into the system, I can’t shrug and blame “bad data.”

The reality: AI won’t just accelerate your team. It will accelerate your blind spots.",Senior Business development leader,Oct 2,Data Quality & Bias,Ai mistakes,https://www.linkedin.com/posts/jacobrholly_why-the-hell-am-i-back-in-school-while-helping-activity-7379338393140461568-OBzM?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Privacy & Compliance,Risks with sensitive data and regulations.,
"Why the hell am I back in school while helping lead an XDR org?

Because if I don’t understand AI deeply, I can’t lead responsibly.

I just wrapped up Part 1 of my AI in Business certification at Arizona State University and one idea stuck with me: bias in AI isn’t a coding bug, it’s a mirror.

If I train a model on our CRM as it is today, it will assume:
Duplicates = real prospects
Missed notes = no activity
Wrong Industry = certain industries always convert

That isn’t the AI making a mistake. That’s our process baked into math.

So is bias a data problem or a leadership problem? Both. But if I’m leading an org that feeds junk into the system, I can’t shrug and blame “bad data.”

The reality: AI won’t just accelerate your team. It will accelerate your blind spots.",Senior Business development leader,Oct 2,ai talent shortage,Ai mistakes,https://www.linkedin.com/posts/jacobrholly_why-the-hell-am-i-back-in-school-while-helping-activity-7379338393140461568-OBzM?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,Privacy & Compliance,Risks with sensitive data and regulations.,
"Your Enterprise AI is Lying to You

LLMs produce confident yet factually incorrect outputs. In consumer apps? Amusing. In enterprises? Devastating.

A major bank's AI told executives their loan default rate was 2.3%.
The actual rate? 4.7%.
That ""small"" error = $200M miscalculation in risk assessment.

The solution isn't abandoning AI. It's making it accountable.
",Founder,sept 30,Data Quality & Bias,Ai errors,https://www.linkedin.com/posts/pdparekh_enterpriseai-aistrategy-datatrust-activity-7379149342462365696-q7tx?utm_source=share&utm_medium=member_desktop&rcm=ACoAAC1KzP8B4Xl6iNe6FF5NuiHM8xL92C-yiJE,,,,,,